{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ddb74f5-5122-4620-a4c4-9e5a653df456",
   "metadata": {},
   "source": [
    "## Einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea6c931-2583-4187-9c22-4a31cc751cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "einsum:\n",
    "    declaration:\n",
    "        A: [K, M]\n",
    "        B: [K, N]\n",
    "        Z: [M, N]\n",
    "    expressions:\n",
    "        - Z[m, n] = A[k, m] * B[k, n]\n",
    "mapping:\n",
    "    rank-order:\n",
    "        A: [K, M]\n",
    "        B: [K, N]\n",
    "        Z: [M, N]\n",
    "    partitioning:\n",
    "        Z:\n",
    "            K: [uniform_shape(K1), uniform_shape(128)]\n",
    "            M: [uniform_shape(M1), uniform_shape(128)]\n",
    "            N: [uniform_shape(N1), uniform_shape(512)]\n",
    "    loop-order:\n",
    "        Z: [M2, N2, K2, M1, N1, K1, M0, N0, K0]\n",
    "    spacetime:\n",
    "        Z:\n",
    "            space: [M0, N0, K0]\n",
    "            time: [M2, N2, K2, M1, N1, K1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5061ab61-212c-445c-9792-87bb4b681c14",
   "metadata": {},
   "source": [
    "## GEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19259f5f-8ed1-4840-aeeb-3350c1f5b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuronxcc.nki as nki\n",
    "import neuronxcc.nki.isa as nisa\n",
    "import neuronxcc.nki.language as nl\n",
    "\n",
    "@nki.jit\n",
    "def nki_gemm(AH, BH, M1=1024, N1 = 1024, K1=1024):\n",
    "    \"\"\"\n",
    "    NKI kernel to compute large matrix-vector multiplication.\n",
    "\n",
    "    Args:\n",
    "        AH: an input tensor of shape [K2, M2]\n",
    "        BH: an input tensor of shape [K2, N2]\n",
    "    Returns:\n",
    "        ZH: the resulting output tensor of shape [M2, N2]\n",
    "    \"\"\"\n",
    "\n",
    "    K2, M2 = AH.shape\n",
    "    K2_, N2 = BH.shape\n",
    "    assert K2 == K2_, \"AH and BH must have the same contraction dimension\"\n",
    "    ZH = nl.ndarray((M2, N2), dtype=AH.dtype, buffer=nl.shared_hbm)\n",
    "\n",
    "    M0 = nl.tile_size.gemm_stationary_fmax  # 128\n",
    "    K0 = nl.tile_size.pmax  # 128\n",
    "    N0 = nl.tile_size.gemm_moving_fmax  # 512\n",
    "\n",
    "    # Blocking large tensors\n",
    "    for m2 in nl.affine_range((M2+M1-1)//M1):\n",
    "        for n2 in nl.affine_range((N2+N1-1)//N1):\n",
    "            ZS = nl.ndarray((M1//M0, nl.par_dim(M0), N1),\n",
    "                          dtype=AH.dtype,\n",
    "                          buffer=nl.sbuf)\n",
    "            for k2 in nl.affine_range((K2+K1-1)//K1):\n",
    "                AS = nl.zeros((K1//K0, M1//M0, nl.par_dim(K0), M0),\n",
    "                                dtype=AH.dtype,\n",
    "                                buffer=nl.sbuf)\n",
    "                BS = nl.zeros((K1//K0, N1//N0, nl.par_dim(K0), N0),\n",
    "                                dtype=BH.dtype,\n",
    "                                buffer=nl.sbuf)\n",
    "\n",
    "                # Loading necessary tiles\n",
    "                i_AS = nl.mgrid[0:K0, 0:M0]\n",
    "                for m1 in nl.affine_range(M1//M0):\n",
    "                    for k1 in nl.affine_range(K1//K0):\n",
    "                        e1 = (K1 * k2 + K0 * k1) + i_AS.p\n",
    "                        e2 = (M1 * m2 + M0 * m1) + i_AS.x\n",
    "                        AS[k1, m1, i_AS.p, i_AS.x] = nl.load(\n",
    "                            AH[e1, e2],\n",
    "                            mask=((e1<K2) & (e2<M2)))\n",
    "\n",
    "                i_BS = nl.mgrid[0:K0, 0:N0]\n",
    "                for n1 in nl.affine_range(N1//N0):\n",
    "                    for k1 in nl.affine_range(K1//K0):\n",
    "                        e1 = (K1 * k2 + K0 * k1) + i_BS.p\n",
    "                        e2 = (N1 * n2 + N0 * n1) + i_BS.x\n",
    "                        BS[k1, n1, i_BS.p, i_BS.x] = nl.load(\n",
    "                        BH[e1,e2], mask=((e1<K2) & (e2<N2)))\n",
    "\n",
    "                # Perform matmul\n",
    "                i_AS_mm = nl.mgrid[0:K0, 0:M0]\n",
    "                i_BS_mm = nl.mgrid[0:K0, 0:N0]\n",
    "                i_ZP_mm = nl.mgrid[0:M0, 0:N0]\n",
    "                for m1 in nl.affine_range(M1//M0):\n",
    "                    for n1 in nl.affine_range(N1//N0):\n",
    "                        ZP = nl.zeros((M0, N0), dtype=nl.float32, buffer=nl.psum)\n",
    "                        for k1 in nl.affine_range(K1//K0):\n",
    "                            ZP[...] += nisa.nc_matmul(\n",
    "                                AS[k1, m1, i_AS_mm.p, i_AS_mm.x],\n",
    "                                BS[k1, n1, i_BS_mm.p, i_BS_mm.x])\n",
    "\n",
    "                        # Accumulate on corresponding SBUF tile\n",
    "                        ZS[m1, i_ZP_mm.p, n1 * N0 + i_ZP_mm.x] += ZP[i_ZP_mm.p, i_ZP_mm.x]\n",
    "\n",
    "            # Copying the result from SBUF to HBM\n",
    "            i_ZS = nl.mgrid[0:M0, 0:N1]\n",
    "            for m1 in nl.affine_range(M1//M0):\n",
    "                e1 = (M1 * m2 + M0 * m1) + i_ZS.p\n",
    "                e2 = (N1 * n2) + i_ZS.x\n",
    "                nl.store(ZH[e1, e2],\n",
    "                         value=ZS[m1, i_ZS.p, i_ZS.x], mask=((e1<M2) & (e2<N2)))\n",
    "                \n",
    "    return ZH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b645d78-c02e-4eb6-928d-ec265c53918c",
   "metadata": {},
   "source": [
    "## AWS GEMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64416555-ca2f-4d5f-855c-c607adc1df3f",
   "metadata": {},
   "source": [
    "The program below is copied directly from the [NKI Matrix Multiplication tutorial](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/tutorials/matrix_multiplication.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277d1e69-cb54-4935-9fbe-7c82c5e008ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuronxcc.nki as nki\n",
    "import neuronxcc.nki.isa as nisa\n",
    "import neuronxcc.nki.language as nl\n",
    "\n",
    "@nki.jit\n",
    "def nki_matmul_fully_optimized_(\n",
    "    lhsT,\n",
    "    rhs,\n",
    "    # Meta-parameters\n",
    "    TILES_IN_BLOCK_M=16,\n",
    "    TILES_IN_BLOCK_N=2,\n",
    "    TILES_IN_BLOCK_K=8,\n",
    "):\n",
    "  \"\"\"NKI kernel to compute a large matrix multiplication efficiently by\n",
    "     blocking all dimensions and doing layout optimization.\n",
    "\n",
    "  Args:\n",
    "      lhsT: an input tensor of shape [K,M], where K is a multiple of 128 *\n",
    "        TILES_IN_BLOCK_K and M is a multiple of 128 * TILES_IN_BLOCK_M.  It is the\n",
    "        left-hand-side argument of the matrix multiplication, delivered transposed\n",
    "        for optimal performance.\n",
    "      rhs: an input tensor of shape [K,N],  where K is a multiple of 128 *\n",
    "        TILES_IN_BLOCK_K and N is a multiple of 512 * TILES_IN_BLOCK_N.  It is\n",
    "        the right-hand-side argument of the matrix multiplication.\n",
    "      TILES_IN_BLOCK_*: meta parameters to control blocking dimensions\n",
    "  Returns:\n",
    "      result: the resulting output tensor of shape [M,N]\n",
    "  \"\"\"\n",
    "\n",
    "  K, M = lhsT.shape\n",
    "  K_, N = rhs.shape\n",
    "  assert K == K_, \"lhsT and rhs must have the same contraction dimension\"\n",
    "  result = nl.ndarray((M, N), dtype=lhsT.dtype, buffer=nl.shared_hbm)\n",
    "\n",
    "  TILE_M = nl.tile_size.gemm_stationary_fmax  # 128\n",
    "  TILE_K = nl.tile_size.pmax  # 128\n",
    "  TILE_N = nl.tile_size.gemm_moving_fmax  # 512\n",
    "\n",
    "  BLOCK_M = TILE_M * TILES_IN_BLOCK_M\n",
    "  BLOCK_N = TILE_N * TILES_IN_BLOCK_N\n",
    "  BLOCK_K = TILE_K * TILES_IN_BLOCK_K\n",
    "\n",
    "  # the size has to be multiple of block size\n",
    "  assert M % BLOCK_M == 0\n",
    "  assert N % BLOCK_N == 0\n",
    "  assert K % BLOCK_K == 0\n",
    "\n",
    "  NUM_BLOCK_M = M // BLOCK_M\n",
    "  NUM_BLOCK_N = N // BLOCK_N\n",
    "  NUM_BLOCK_K = K // BLOCK_K\n",
    "\n",
    "  # Blocking N dimension (the RHS free dimension)\n",
    "  for n in nl.affine_range(NUM_BLOCK_N):\n",
    "    result_tiles = nl.zeros((NUM_BLOCK_M, TILES_IN_BLOCK_M, TILES_IN_BLOCK_N,\n",
    "                             nl.par_dim(TILE_M), TILE_N),\n",
    "                            dtype=lhsT.dtype,\n",
    "                            buffer=nl.sbuf)\n",
    "\n",
    "    # Blocking K dimension (the contraction dimension)\n",
    "    # Use `sequential_range` because we do not want the compiler to change this loop by, \n",
    "    # for example, vectorizing it\n",
    "    for k in nl.sequential_range(NUM_BLOCK_K):\n",
    "      # Loading tiles from rhs\n",
    "      # setting the load tile to `TILE_K x BLOCK_SIZE_N` to optimize DMA performance\n",
    "      i_rhs = nl.mgrid[0:TILE_K, 0:BLOCK_N]\n",
    "      rhs_tiles = nl.ndarray((TILES_IN_BLOCK_K, nl.par_dim(TILE_K), BLOCK_N),\n",
    "                             dtype=rhs.dtype,\n",
    "                             buffer=nl.sbuf)\n",
    "\n",
    "      for bk_r in nl.affine_range(TILES_IN_BLOCK_K):\n",
    "        rhs_tiles[bk_r, i_rhs.p, i_rhs.x] = nl.load(\n",
    "            rhs[(TILES_IN_BLOCK_K * k + bk_r) * TILE_K + i_rhs.p,\n",
    "                BLOCK_N * n + i_rhs.x])\n",
    "\n",
    "      # Blocking M dimension (the LHS free dimension)\n",
    "      for m in nl.affine_range(NUM_BLOCK_M):\n",
    "        # Loading tiles from lhsT\n",
    "        i_lhsT = nl.mgrid[0:TILE_K, 0:BLOCK_M]\n",
    "        lhsT_tiles = nl.ndarray((TILES_IN_BLOCK_K, nl.par_dim(TILE_K), BLOCK_M),\n",
    "                                dtype=lhsT.dtype,\n",
    "                                buffer=nl.sbuf)\n",
    "        for bk_l in nl.affine_range(TILES_IN_BLOCK_K):\n",
    "          lhsT_tiles[bk_l, i_lhsT.p, i_lhsT.x] = nl.load(\n",
    "              lhsT[(TILES_IN_BLOCK_K * k + bk_l) * TILE_K + i_lhsT.p,\n",
    "                   BLOCK_M * m + i_lhsT.x])\n",
    "\n",
    "        # Do matmul with all tiles in the blocks\n",
    "        i_lhsT_mm = nl.mgrid[0:TILE_K, 0:TILE_M]\n",
    "        i_rhs_mm = nl.mgrid[0:TILE_K, 0:TILE_N]\n",
    "        i_res_mm = nl.mgrid[0:TILE_M, 0:TILE_N]\n",
    "        for bn in nl.affine_range(TILES_IN_BLOCK_N):\n",
    "          for bm in nl.affine_range(TILES_IN_BLOCK_M):\n",
    "            res_tile = nl.zeros((TILE_M, TILE_N), dtype=nl.float32, buffer=nl.psum)\n",
    "\n",
    "            for bk in nl.affine_range(TILES_IN_BLOCK_K):\n",
    "              res_tile[...] += nisa.nc_matmul(\n",
    "                  lhsT_tiles[bk, i_lhsT_mm.p, bm * TILE_M + i_lhsT_mm.x],\n",
    "                  rhs_tiles[bk, i_rhs_mm.p, bn * TILE_N + i_rhs_mm.x])\n",
    "\n",
    "            # Accumulate on corresponding SBUF tile\n",
    "            result_tiles[m, bm, bn, i_res_mm.p,\n",
    "                         i_res_mm.x] += res_tile[i_res_mm.p, i_res_mm.x]\n",
    "\n",
    "    # Copying the result from SBUF to HBM\n",
    "    for m in nl.affine_range(NUM_BLOCK_M):\n",
    "      for bm in nl.affine_range(TILES_IN_BLOCK_M):\n",
    "        i_res = nl.mgrid[0:TILE_K, 0:TILE_N]\n",
    "        i_res_packed = nl.mgrid[0:TILE_K, 0:BLOCK_N]\n",
    "        result_packed = nl.ndarray((TILE_K, BLOCK_N),\n",
    "                                   dtype=result_tiles.dtype,\n",
    "                                   buffer=nl.sbuf)\n",
    "\n",
    "        # coalesce result tiles for better DMA performance\n",
    "        for bn in nl.affine_range(TILES_IN_BLOCK_N):\n",
    "          result_packed[i_res.p,\n",
    "                        bn * TILE_N + i_res.x] = nl.copy(result_tiles[m, bm, bn,\n",
    "                                                                      i_res.p,\n",
    "                                                                      i_res.x])\n",
    "        nl.store(result[(TILES_IN_BLOCK_M * m + bm) * TILE_K + i_res_packed.p,\n",
    "                        BLOCK_N * n + i_res_packed.x],\n",
    "                 value=result_packed[i_res_packed.p, i_res_packed.x])\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9792da2-361a-43d1-b1fd-8be6c0f8fe7f",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb627c4-093b-4cc6-90f3-1946b89cba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gemm import nki_gemm\n",
    "\n",
    "# Set matrix dimensions\n",
    "# K = 1024\n",
    "# M = 4096\n",
    "# N = 2048\n",
    "\n",
    "K = 1500\n",
    "M = 4500\n",
    "N = 2500\n",
    "\n",
    "# Test random matrices\n",
    "for i in range(5):\n",
    "    A = np.random.rand(K, M).astype(np.float16)\n",
    "    B = np.random.rand(K, N).astype(np.float16)\n",
    "    result_nki = nki_gemm(A, B)\n",
    "    result_np = np.dot(A.T, B)\n",
    "    is_close = np.allclose(result_nki, result_np, rtol=1e-2, atol=1e-4)\n",
    "    print(\"Result match: \", is_close)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060aec91-dd20-4fc9-9d3b-d03c0d352776",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17098c62-d72f-472f-a38b-62bd6a25cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuronxcc.nki as nki\n",
    "import numpy as np\n",
    "from gemm import nki_gemm\n",
    "from aws_gemm import nki_matmul_fully_optimized_\n",
    "\n",
    "K = 16384\n",
    "M = 8192\n",
    "N = 16384\n",
    "\n",
    "A = np.random.rand(K, M).astype(np.float16)\n",
    "B = np.random.rand(K, N).astype(np.float16)\n",
    "\n",
    "def benchmark_nki(nki_func):\n",
    "    bench_func = nki.benchmark(warmup=5, iters=10)(nki_func)\n",
    "    bench_func(A, B)\n",
    "    latency_res = bench_func.benchmark_result.nc_latency\n",
    "    p99 = latency_res.get_latency_percentile(99)\n",
    "    print(\"Latency: {:.2f} ms (P99)\".format(p99 / 1000.0))\n",
    "\n",
    "print(\"Benchmarking aws_gemm\")\n",
    "benchmark_nki(nki_matmul_fully_optimized_)\n",
    "\n",
    "print(\"Benchmarking gemm\")\n",
    "benchmark_nki(nki_gemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d74d67-db45-45f7-bde0-bbcedfb0105a",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708af63a-e0b4-4c97-bb50-26adf4c0eb62",
   "metadata": {},
   "source": [
    "The latency of both gemm functions above, one following the TeAAL specification another taken directly from AWS's NKI tutorial, are comparable to one another. However, `nki_gemm` is generally slightly faster than `nki_matmul_fully_optimized_`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
