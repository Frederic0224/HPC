{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4e7a72-30d4-4b7e-8db6-d30d07822825",
   "metadata": {},
   "source": [
    "# Iterative Blocked Householder QR Using TSQR Panel Factorization\n",
    "\n",
    "In this notebook, we will introduce the mathematical background for an iterative blocked Householder QR algorithm that implements tall-skinny QR (TSQR) for panel factorization. We will also attempt to represent the algorithm using cascades of Einsums. While the algorithms here only work for real matrices, all concepts can be easily generalized to complex matrices using Hermitian transposes.\n",
    "\n",
    "**This notebook is primarily based on the following papers:**\n",
    "- [Reconstructing Householder Vectors from Tall-Skinny QR](https://ieeexplore.ieee.org/document/6877344)\n",
    "- [High Performance Householder QR Factorization on Emerging GPU Architectures Using Tensor Cores](https://ieeexplore.ieee.org/document/10816084)\n",
    "- [High Accuracy Matrix Computations on Neural Engines: A Study of QR Factorization and its Applications](https://dl.acm.org/doi/10.1145/3369583.3392685)\n",
    "- [A Storage-Efficient WY Representation for Products of Householder Transformations](https://epubs.siam.org/doi/10.1137/0910005)\n",
    "\n",
    "Some modification to [High Performance Householder QR Factorization on Emerging GPU Architectures Using Tensor Cores](https://ieeexplore.ieee.org/document/10816084) are introduced below to improve memory efficiency. Specifically, rather than using the traditional WY transform, the CWY Householder product representation is used.\n",
    "\n",
    "**Note the following:**\n",
    "- Einsum notations in this notebook follow [The EDGE Language: Extended General Einsums for Graph Algorithms](https://arxiv.org/abs/2404.11591).\n",
    "- While the numpy code does not reflect this, the $\\text{sign}$ function should follow the convention that $\\text{sign}(0) = 1$ for both real and complex values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36351613-0d90-4302-9571-f96ff4661231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74a53c-f254-49df-9792-d9406ff97462",
   "metadata": {},
   "source": [
    "We first generate random matrices that will be used to test our QR factorization implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54653c6d-99f7-45f7-8212-07fb8a97caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "randMatrices = []\n",
    "for i in range(50):\n",
    "    randMatrices.append(np.random.rand(150,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0984fc31-96cf-421f-88ed-069cee09f124",
   "metadata": {},
   "source": [
    "For QR decomposition, one is generally concerned with backward and orthogonality error. To ensure the correctness of our QR implementation, we will also check if matrix $R$ is actually upper triangular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a5350d-783e-470b-bd71-85b3884b3510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardError(matrix, q, r):\n",
    "    \"\"\"Calculates the backward error.\"\"\"\n",
    "    error = np.linalg.norm(matrix - np.dot(q, r), ord=2) / np.linalg.norm(matrix, ord=2)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79057567-41b8-4175-9257-126929740a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonalityError(q):\n",
    "    \"\"\"Calculates the orthogonality error of matrix Q.\"\"\"\n",
    "    error = np.linalg.norm(np.eye(q.shape[1]) - np.dot(q.T, q), ord=2)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce61d7-9224-4975-b5c8-f72e21339e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_upper_triangular(R, tol=1e-14):\n",
    "    \"\"\"Determines if matrix R is upper triangular.\"\"\"\n",
    "    lower_triangle_mask = np.tril_indices_from(R, k=-1)\n",
    "    return np.all(np.abs(R[lower_triangle_mask]) < tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af629567-8d89-4fe4-9ee4-43a5d3262453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testQR(matrices, func):\n",
    "    \"\"\"Test to check if FUNC correctly computes the QR factorization for all matrices in MATRICES.\"\"\"\n",
    "    totalTimeCost = 0\n",
    "    totalBackwardError = 0\n",
    "    totalOrthogonalityError = 0\n",
    "    \n",
    "    for matrix in matrices:\n",
    "        start = time.perf_counter()\n",
    "        q, r = func(matrix)\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        totalTimeCost += (end - start)\n",
    "        \n",
    "        backwardE = backwardError(matrix, q, r)\n",
    "        totalBackwardError += backwardE\n",
    "\n",
    "        orthogonalityE = orthogonalityError(q)\n",
    "        totalOrthogonalityError += orthogonalityE\n",
    "\n",
    "        isUpperTriangular = is_upper_triangular(r)\n",
    "\n",
    "        # Tolerance levels are set to 1e-12 although the errors will not usually be this large\n",
    "        if not (backwardE < np.float64(1e-12) and orthogonalityE < np.float64(1e-12) and isUpperTriangular):\n",
    "            print(\"Test Failed\")\n",
    "            print(\"Average time:\" + str(totalTimeCost/len(matrices)))\n",
    "            print(\"Backward error: \" + str(backwardE))\n",
    "            print(\"Orthogonality error: \" + str(orthogonalityE))\n",
    "            print(\"Is upper triangular: \" + str(isUpperTriangular))\n",
    "            return\n",
    "\n",
    "    print(\"Test Passed.\")\n",
    "    print(\"Average time:\" + str(totalTimeCost/len(matrices)))\n",
    "    print(\"Average backward error:\" + str(totalBackwardError/len(matrices)))\n",
    "    print(\"Average orthogonality error:\" + str(totalOrthogonalityError/len(matrices)))\n",
    "    print(\"Is upper triangular: \" + str(isUpperTriangular))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14040736-95b1-4713-8f22-45a88eb6d8d0",
   "metadata": {},
   "source": [
    "To check if our test is functional, we first test `np.linalg.qr` and `scipy.linalg.qr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1696bb-1d52-4790-8200-478f189eff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "testQR(randMatrices, np.linalg.qr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb5a30e-5eec-4248-9aa6-304665bf250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testQR(randMatrices, scipy.linalg.qr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219fa39a-a35f-48c7-b0b8-3e7346ba9928",
   "metadata": {},
   "source": [
    "We start by implementing the traditional unblocked Householder QR factorization. The implementation is based on the Householder QR algorithm introduced in [Advanced Linear Algebra: Foundations to Frontiers](https://www.cs.utexas.edu/~flame/laff/alaff/chapter03-real-HQR-factorization.html).\n",
    "\n",
    "Let $A$ be the input matrix, where $A \\in \\mathbb{R}^{m \\times n}$ and $m \\ge n$. The thin QR decomposition computes matrices $Q$ and $R$, where:\n",
    "- $Q \\in \\mathbb{R}^{m \\times n}$ (thin orthogonal matrix)\n",
    "- $R \\in \\mathbb{R}^{n \\times n}$ (upper triangular matrix).\n",
    "\n",
    "We introduce three-tensor representations for iteration tracking.\n",
    "- $R^{I,M,N}$. Let $R^{I,M,N}_{0, m, n} = A_{m,n}$.\n",
    "- $Q^{I,M,N}$. Let $Q^{I,M,N}_{0, m, n} = I_{m,n}$.\n",
    "- Let unspecified indices take on default values of $0$.\n",
    "\n",
    "Index info:\n",
    "- $i$: iterative rank that ranges from $[0,n-1]$.\n",
    "\n",
    "During iteration $i$, the Householder vector and transformation are computed as follows.\n",
    "\n",
    "Step 1: Generate the Householder vector.\n",
    "$$\\chi_1 = R^{I,M,N}_{i,i,i}$$\n",
    "$$\\chi_2 = \\sqrt{R^{I,M,N}_{i, m:m \\ge i+1, i}R^{I,M,N}_{i, m:m \\ge i+1, i}}$$\n",
    "$$\\alpha = \\sqrt{\\chi_1^2 + \\chi_2^2}$$\n",
    "$$\\nu = \\chi_1 + \\text{sign}(\\chi_1)\\alpha$$\n",
    "$$V_{i,m:m \\ge i+1} = \\frac{R^{I,M,N}_{i, m:m \\ge i+1, i}}{\\nu}$$\n",
    "$$\\chi_2 = \\frac{\\chi_2}{\\text{abs}(\\nu)}$$\n",
    "$$\\tau = \\frac{1 + \\chi_2^2}{2}$$\n",
    "\n",
    "Step 2: Update the $Q$ and $R$ matrices.\n",
    "$$\n",
    "U_{i,m} =\n",
    "\\begin{cases} \n",
    "0, & \\text{if } m < i, \\\\\n",
    "1, & \\text{if } m = i, \\\\\n",
    "V_{i,m}, & \\text{if } m > i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$P_{i,m,n} = \\frac{1}{\\tau} U_{i,m}U^{I,M}_{i,n}$$\n",
    "$$G_{i,m,n} = P^{I,M,N}_{i,m,k}R^{I,M,N}_{i, k, n}$$\n",
    "$$R_{i+1,m,n} = R_{i,m,n} - G_{i,m,n}$$\n",
    "\n",
    "$$T_{i,m,n} = Q^{I,M,N}_{i,m,k}P^{I,M,N}_{i,k,n}$$\n",
    "$$Q^{I,M,N}_{i+1,m,n} = Q^{I,M,N}_{i,m,n} - T_{i,m,n}$$\n",
    "\n",
    "The final output of the algorithm will be:\n",
    "- $Q = Q^{I,M,N}_{N+1, m,n}$\n",
    "- $R = R^{I,M,N}_{N+1, m,n}$\n",
    "\n",
    "Because $U$ contains $0$ for all $m<i$\n",
    "$$P=\\begin{bmatrix}\n",
    "        0 & 0 \\\\\n",
    "        0 & P_4\n",
    "    \\end{bmatrix},$$ so $P_4$ is applied to update $R$ and $Q$ so we can update $Q$ and $R$ as follows.\n",
    "$$PR = \\begin{bmatrix}\n",
    "        0 & 0 \\\\\n",
    "        0 & P_4\n",
    "    \\end{bmatrix}\\begin{bmatrix}\n",
    "        R_1 & R_2 \\\\\n",
    "        0 & R_4\n",
    "    \\end{bmatrix} = \\begin{bmatrix}\n",
    "        0 &  0\\\\\n",
    "        0 & P_4R_4\n",
    "    \\end{bmatrix}$$\n",
    "$$QP = \\begin{bmatrix}\n",
    "        Q_1& Q_2\n",
    "    \\end{bmatrix}\\begin{bmatrix}\n",
    "        0 & 0 \\\\\n",
    "        0 & P_4\n",
    "    \\end{bmatrix} = \\begin{bmatrix}\n",
    "        0 & Q_2P_4\n",
    "    \\end{bmatrix}$$\n",
    "\n",
    "Thus, the program can be made efficient by avoiding ineffectual operations.\n",
    "\n",
    "Notice that in the cascade of Einsums here, the Householder matrix is not explicitly form but applied implicitly. In the `unblockedHouseholderQR` function below, the Householder matrix will still be explicitly formed but implicit update will be used from `blockedHouseholderQR` onwards for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35ca84-a629-46b3-a6a7-58505fcf3b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def householderReflector(x):\n",
    "    \"\"\"Computes the Householder vector given an input vector.\"\"\"\n",
    "    chi_1 = x[0]\n",
    "    chi_2 = np.linalg.norm(x[1:], ord=2) # np.sqrt(np.einsum('i,i->', x[1:], x[1:]))\n",
    "\n",
    "    alpha = np.sqrt(chi_1**2 + chi_2**2) # 2-norm of the input vector\n",
    "\n",
    "    # Construct the Householder vector below the first element\n",
    "    v = chi_1 + np.sign(chi_1) * alpha\n",
    "    u_2 = x[1:] / v\n",
    "    chi_2 /= np.abs(v)\n",
    "    \n",
    "    # Compute the tau constant\n",
    "    tau = 0.5 * (1 + chi_2 ** 2)\n",
    "    \n",
    "    return u_2, tau\n",
    "\n",
    "def unblockedHouseholderQR(A):\n",
    "    \"\"\"Computes the unblocked Householder QR factorization for input matrix A.\"\"\"\n",
    "    R = np.copy(A)\n",
    "    m, n = A.shape\n",
    "    Q = np.eye(m)\n",
    "\n",
    "    for i in range(n):\n",
    "        # Construct the Householder vector and its corresponding tau constant\n",
    "        u_2, tau = householderReflector(R[i:,i])\n",
    "        u = np.concatenate(([1], u_2))\n",
    "        \n",
    "        # Build the Householder matrix H\n",
    "        H = np.eye(u.shape[0]) - 1/tau * np.outer(u,u) # np.einsum('i,j->ij', u, u)\n",
    "        \n",
    "        # Apply the Householder transformation to Q and R matrices\n",
    "        R[i:,i:] = H @ R[i:,i:] # np.einsum('ij,jk->ik', H, R[i:,i:])\n",
    "        Q[:,i:] = Q[:,i:] @ H # np.einsum('ij,jk->ik', Q[:,i:], H)\n",
    "    \n",
    "    Q = Q[:,:n]\n",
    "    R = R[:n,:]\n",
    "    return Q, R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c984b1a2-b885-4f35-bc61-43c50124666a",
   "metadata": {},
   "source": [
    "For matrices with a shape larger than 100 in either dimension, the `unblockedHouseholderQR` can be really slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82d3e96-3ee1-4b5c-8c14-c2163f99b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testQR(randMatrices, unblockedHouseholderQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1710e-6c51-47bc-a780-127bc6527ea4",
   "metadata": {},
   "source": [
    "Next, we will implement blocked Householder QR via compact WY transform. The implementation is based on the paper [A Storage-Efficient WY Representation for Products of Householder Transformations](https://epubs.siam.org/doi/10.1137/0910005).\n",
    "\n",
    "The compact WY (CWY) transform involves matrix $Y$ and $T$. For an input panel matrix of shape $A \\in \\mathbb{R}^{m \\times n}$, $Y \\in \\mathbb{R}^{m \\times n}$  and $T \\in \\mathbb{R}^{n \\times n}$. By construction, $Y$ is lower triangular, $T$ is upper triangular, and $Q=I-YTY^T \\in \\mathbb{R}^{m \\times m}$. Compared to the standard WY transform, as documented in [The WY Representation for Products of Householder Matrices](https://epubs.siam.org/doi/abs/10.1137/0908009), CWY is more memory efficient. The WY representation encodes Householder products using two matrices, $W \\in \\mathbb{R}^{m \\times n}$ and $Y \\in \\mathbb{R}^{m \\times n}$. In the case where $m \\gg n$, which is usually the case for when performing panel factorization, the compact WY representation is generally more preferable. In general the CWY representation is also more preferable as many linear algebra packets also implements QR decomposition using CWY transform.\n",
    "\n",
    "We will proceed to explain the CWY representation briefly.\n",
    "Let $Q$ be the product of $n$ Householder matrices. We define $Q_+$ as\n",
    "$$Q_+ = QH_{n+1},$$\n",
    "where $H_{n+1}$ is the $[n+1]$-th Householder matrix. Recall that the Householder reflector is defined as $H_{n+1} = I-\\frac{u_{n+1}u_{n+1}^T}{\\tau_{n+1}}$. where $u_{n+1}$ is the Householder reflector. For brevity, let $u=u_{n+1}$ and $\\tau = \\tau_{n+1}$ below.\n",
    "\n",
    "We want to show that\n",
    "$$Q_+ = QH_{n+1} = (I-YTY^T)H_{n+1} = I-Y_+T_+Y_+^T,$$\n",
    "where\n",
    "$$Y_+ = [Y, u] \\in \\mathbb{R}^{m \\times \\{n + 1\\}}$$\n",
    "and \n",
    "$$T_+ = \\left[\n",
    "\\begin{array}{c c}\n",
    "T & z \\\\\n",
    "0 & \\rho\n",
    "\\end{array}\n",
    "\\right] \\in \\mathbb{R}^{\\{n + 1\\} \\times \\{n + 1\\}}$$\n",
    "with $\\rho = \\frac{1}{\\tau}$, and $z = -\\rho TY^Tu$.\n",
    "\n",
    "To prove this, see that,\n",
    "$$\\begin{align*}\n",
    "    I - Y_+ T_+ Y_+^T &= I - \n",
    "    \\begin{bmatrix}\n",
    "        Y & u\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        T & z \\\\\n",
    "        0 & \\rho\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        Y^T \\\\\n",
    "        u^T\n",
    "    \\end{bmatrix} \\\\[8pt]\n",
    "    &= I - \n",
    "    \\begin{bmatrix}\n",
    "        Y & u\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        TY^T + zu^T \\\\\n",
    "        \\rho u^T\n",
    "    \\end{bmatrix} \\\\[8pt]\n",
    "    &= I - YTY^T - Yzu^T - \\rho uu^T \\\\\n",
    "    &= I - YTY^T + \\frac{1}{\\tau} YTY^Tuu^T - \\frac{1}{\\tau}uu^T.\n",
    "\\end{align*}$$\n",
    "\n",
    "Note that\n",
    "$$Q_+ = QH_{n+1} = (I-YTY^T)(I-\\frac{1}{\\tau}uu^T)=I-YTY^T+\\frac{1}{\\tau}YTY^Tuu^T-\\frac{1}{\\tau}uu^T.$$\n",
    "\n",
    "Thus, we have proven that the CWY representation is valid by induction as the base case is trivially true given that the initial construction for the $Y$ and $T$ matrices is just:\n",
    "$$Y = [u_1]$$\n",
    "and\n",
    "$$T = \\begin{bmatrix}\n",
    "            \\frac{1}{\\tau_1}\n",
    "        \\end{bmatrix},$$\n",
    "where $u_1$ and $\\tau_1$ can be used to form $H_1$, the first Householder reflector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2950d42f-ce9b-4eea-a6cc-f5fe041d0545",
   "metadata": {},
   "source": [
    "If a complete $Q$ is necessary, $Q=I-YTY^T \\in \\mathbb{R}^{m \\times m}$. If a thin $Q$ suffices, then $Q = I-YTY_1^T \\in \\mathbb{R}^{m \\times n}$, where $Y_1$ is the upper square section of $Y$. This definition of $Y_1$ will continue to be used throughout this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e57fae-f9fb-406b-900f-b9ce3c3320e6",
   "metadata": {},
   "source": [
    "Rather than using purely sequential updates, as shown above, which mainly involves level 2 BLAS operations, we will attempt to increase the use of matrix multiplications.\n",
    "\n",
    "After computing $Y_{new}$ and $T_{new}$ for a panel in `panelQR`, we now have\n",
    "$$T=\\begin{bmatrix}\n",
    "        T_{\\text{old}} & 0 \\\\\n",
    "        0 & T_{\\text{new}}\n",
    "    \\end{bmatrix}.$$\n",
    "$$Y=\\begin{bmatrix}\n",
    "        Y_{\\text{old}} & Y_{\\text{new}}\n",
    "    \\end{bmatrix}.$$\n",
    "\n",
    "We will need to update $T$ to ensure global correctness. We consider\n",
    "$$T=\\begin{bmatrix}\n",
    "        T_{\\text{old}} & T_{\\text{cross}} \\\\\n",
    "        0 & T_{\\text{new}}\n",
    "    \\end{bmatrix}.$$\n",
    "\n",
    "Our goal is to determine $T_{\\text{cross}}$.\n",
    "From the recursive definition of compact WY transformation along with zero padding to ensure matching matrix dimensions, we have:\n",
    "$$\\begin{align}\n",
    "Q_{\\text{old}}Q_{\\text{new}}&=(I-Y_{\\text{old}}T_{\\text{old}}Y^T_{\\text{old}})(I-Y_{\\text{new}}T_{\\text{new}}Y^T_{\\text{new}})\\\\\n",
    "&= I - Y_{\\text{new}}T_{\\text{new}}Y^T_{\\text{new}} - Y_{\\text{old}}T_{\\text{old}}Y^T_{\\text{old}} + Y_{\\text{old}}T_{\\text{old}}Y^T_{\\text{old}}Y_{\\text{new}}T_{\\text{new}}Y^T_{\\text{new}}\\\\\n",
    "&= I-YTY^T\\\\\n",
    "&= I - Y_{\\text{new}}T_{\\text{new}}Y^T_{\\text{new}} - Y_{\\text{old}}T_{\\text{old}}Y^T_{\\text{old}} - Y_{\\text{old}}T_{\\text{cross}}Y^T_{\\text{new}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We observe that the definition such that\n",
    "$$T_{\\text{cross}}=-T_{\\text{old}}Y^T_{\\text{old}}Y_{\\text{new}}T_{\\text{new}}$$\n",
    "captures the interaction between old and new reflectors, ensuring that the blocked structure maintains the recursive formulation of the compact WY representation. This concludes how blocked updates can be generalized to ensure global correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64a2357-5de4-44df-8ccb-72b4e46fefd7",
   "metadata": {},
   "source": [
    "We will attempt to represent the blocked Householder QR algorithms in terms of Einsum cascades.\n",
    "\n",
    "Initially:\n",
    "- $R^{I,M,N}_{0, m, n} = A_{m,n}$.\n",
    "- $Y^{I,M,N}$\n",
    "- $T^{J,K,N,P}_{0,0,n,p} = 0^{n \\times n}$\n",
    "\n",
    "Iteratively, `Y` and `T` will be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60902dda-c5a0-45df-a96e-00ccf5cd3452",
   "metadata": {},
   "source": [
    "Index info:\n",
    "- We want to build matrix $Y$ and $T$ while updating $R$ along rank $N$ iteratively. For `panelQR` we want to partition the iterative $N$ into an $N1$ and an $N0$.\n",
    "- Let the iterative $N1$ be $K$ and the iterative $N0$ be $J$.\n",
    "\n",
    "$$\\text{Furthermore, let the panel be } \\tilde{A}^{J,K,M,N}_{0,k,m,n}=A^{M,N}_{m:m\\ge k * J, n:k*J \\le n < (k+1)J}$$\n",
    "**Please remember that the $\\tilde{A} \\ne A$. $\\tilde{A}$ is a panel of the input matrix $A$.**\n",
    "\n",
    "Let $V$ and $\\tau$ be the values returned by calling `householderReflector`. The definition of $V$ and $\\tau$ as well as the `householderReflector` function follows the the Einsum shown in the unblocked Householder QR section. Recall that `householderReflector` returns elements following the first element in the Householder vector and the tau constant.\n",
    "\n",
    "Let's consider a fixed value for $k$ and $j$. We will also omit $k$ and $j$ iterative ranks for brevity.\n",
    "\n",
    "Step 1: Construct the Householder vector\n",
    "$$\n",
    "U_{m} =\n",
    "\\begin{cases}\n",
    "0, & \\text{if } m < kJ + j, \\\\\n",
    "1, & \\text{if } m = kJ + j, \\\\\n",
    "V_{m-1}, & \\text{if } m > kJ + j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Step 2: Update $Y$\n",
    "\n",
    "$$Y^{M,N}_{m,kJ + j} = U_{m}$$\n",
    "\n",
    "Step 3: Update $\\tilde{A}$\n",
    "$$\\rho = \\frac{1}{\\tau}$$\n",
    "$$P_{m,n} = \\rho U_{m}U^{M}_{n}$$\n",
    "$$G_{m,n} = \n",
    "\\begin{cases}\n",
    "P^{M,N}_{m,l}\\tilde{A}^{M,N}_{l,n}, & \\text{if } (n \\ge kJ+j),\\\\\n",
    "0, & \\text{else}\n",
    "\\end{cases}$$\n",
    "$$\\text{Update $j$ to $j+1$: }\\tilde{A}_{m,n} = \\tilde{A}_{m,n} - G_{m,n}$$\n",
    "\n",
    "Step 4: Update $T$\n",
    "\n",
    "For this step, the bounds are defined as:\n",
    "- $m \\ge kJ + j$\n",
    "- $kJ \\le n,p < kJ+j$\n",
    "- Unspecified elements take the default value of $0$.\n",
    "$$B_{n} = Y_{m,n}U_{m}$$\n",
    "$$Z_{n} = -\\rho T_{n,p}B^{N}_{p}$$\n",
    "$$T_{n,p} =\n",
    "\\begin{cases}\n",
    "\\rho, & \\text{if } (n = p = kJ+j),\\\\\n",
    "Z_{n}, & \\text{if } (p = kJ+j) \\wedge (n < p),\\\\\n",
    "T_{n,p}, & \\text{else}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1392a60-8e66-4bb8-ac00-2f7c1fa9df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def householderReflector(x):\n",
    "    \"\"\"Computes the Householder vector given an input vector.\"\"\"\n",
    "    chi_1 = x[0]\n",
    "    chi_2 = np.linalg.norm(x[1:], ord=2) # np.sqrt(np.einsum('i,i->', x[1:], x[1:]))\n",
    "\n",
    "    alpha = np.sqrt(chi_1**2 + chi_2**2) # 2-norm of the input vector++++\n",
    "\n",
    "    # Construct the Householder vector below the first element\n",
    "    v = chi_1 + np.sign(chi_1) * alpha\n",
    "    u_2 = x[1:] / v\n",
    "    chi_2 /= np.abs(v)\n",
    "    \n",
    "    # Compute the tau constant\n",
    "    tau = 0.5 * (1 + chi_2 ** 2)\n",
    "    \n",
    "    return u_2, tau\n",
    "\n",
    "\n",
    "def panelQR(A, Y, T, k):\n",
    "    \"\"\"Update the Y,T matrices for a block of matrix A.\"\"\"\n",
    "    m, n = A.shape\n",
    "    \n",
    "    for i in range(n):\n",
    "        idx = k + i  # Relative index in Y and T\n",
    "\n",
    "        # Compute Householder vector and tau\n",
    "        Y[idx+1:, idx], tau = householderReflector(A[i:, i])\n",
    "        u = Y[idx:, idx]\n",
    "\n",
    "        # Apply Householder transformation to reduce panel A\n",
    "        rho = 1 / tau\n",
    "        A[i:, i:] -= rho * np.outer(u, np.dot(u, A[i:, i:])) # np.einsum('i,j->ij', u, np.einsum('i,ij->j', u, A[i:, i:]))\n",
    "\n",
    "        # Update the T matrix\n",
    "        T[idx, idx] = rho\n",
    "        if i > 0:\n",
    "            z = T[k:idx, k:idx] @ Y[idx:, k:idx].T @ u\n",
    "            T[k:idx, idx] = -rho * z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6924548f-77d2-439b-bf94-a51ff4ada036",
   "metadata": {},
   "source": [
    "We will continue to discuss the `blockedHouseholderQR` function. The convention for variables follow the previous section on `panelQR`.\n",
    "\n",
    "In this function, we will call `panelQR` on $\\tilde{A}$ to compute the Householder QR of the panel and generalize the $Y$ and $T$ matrices so they are correct globally.\n",
    "\n",
    "**m and n are not restricted to a certain range of values here. Furthermore, we will only omit the $J$ iterative rank here. Rank $K$ is needed.**\n",
    "\n",
    "To update the global $Y$ and $T$ matrices as well as the trailing matrix, we compute:\n",
    "\n",
    "$$C_{k,m,n}=Y^{K,M,N}_{q: q < k, l:l \\ge kJ,m}Y^{K,M,N}_{k, l,n}$$\n",
    "$$D_{k,m,p}=C_{k,m,n}T^{K,N,P}_{k,n:kJ\\le n < (k+1)J,p}$$\n",
    "$${E}_{k,n,m}=-T^{K,N,P}_{q:q<k, n:n<kJ, p}D^{K,M,P}_{k,p,m}$$\n",
    "$$T_{k,n,p}=E^{K,N,M}_{k,n,p}$$\n",
    "$$F_{q,l,n}=\n",
    "\\begin{cases} \n",
    "Y^{K,M,N}_{k,p:p\\ge kJ,l}R^{K,M,N}_{q,p,n}, & \\text{if } (kJ \\le l < (k+1)J) \\wedge (q\\ge k+1), \\\\\n",
    "0, & \\text{else}\n",
    "\\end{cases}\n",
    "$$\n",
    "$$W_{q,m,n} = T^{K,N,P}_{k,l,m}F_{q:q\\ge k+1,l,n}$$\n",
    "$$\\text{Update $k$ to $k+1$: }R_{q,m,n} = R_{q,m,n} - Y^{K,M,N}_{k,m,t}W^{K,M,N}_{q:q\\ge k+1,t,n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d791cd-57b8-47b3-ac1b-323bc9ee7436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blockedHouseholderQR(A, block_size=30, complete=False, householder_repr = False):\n",
    "    \"\"\"Computes a blocked Householder QR factorization using Compact WY representation.\"\"\"\n",
    "    m, n = A.shape\n",
    "    R = np.copy(A)\n",
    "    Y = np.eye(m, n)\n",
    "    T = np.zeros((n, n))\n",
    "    \n",
    "    for k in range(0, n, block_size):\n",
    "        bk = min(block_size, n - k)  # Block size\n",
    "\n",
    "        # Compute a panel of the input matrix\n",
    "        panelQR(R[k:, k:k+bk], Y, T, k)\n",
    "\n",
    "        if k != 0:\n",
    "            # Compute T_cross\n",
    "            T_cross = -T[:k, :k] @ (Y[k:, :k].T @ Y[k:, k:k+bk]) @ T[k:k+bk, k:k+bk] # -np.einsum(\"ij,jk,kl,lm->im\", T[:k, :k], Y[k:, :k].T, Y[k:, k:k+bk], T_panel)\n",
    "            # Update the global T matrix\n",
    "            T[:k, k:k+bk] = T_cross\n",
    "\n",
    "        # Trailing matrix update\n",
    "        W = T[k:k+bk, k:k+bk].T @ Y[k:, k:k+bk].T @ R[k:, k+bk:] # np.einsum('ji,jk->ik', T[k:k+bk, k:k+bk], np.einsum('ji,jk->ik', Y[k:, k:k+bk], R[k:, k+bk:]))\n",
    "        R[k:, k+bk:] -= np.dot(Y[k:, k:k+bk], W) # np.einsum('ij,jk->ik', Y[k:, k:k+bk], W)\n",
    "\n",
    "    if householder_repr:\n",
    "        return Y, T, R\n",
    "    elif complete:\n",
    "        return np.eye(m, m) - np.dot(Y, np.dot(T, Y.T)), R # np.einsum('ij,jk,kl->il', Y, T, Y.T), R\n",
    "    else:\n",
    "        return np.eye(m, n) - np.dot(Y, np.dot(T, Y[:n, :].T)), R[:n, :] # np.einsum('ij,jk,kl->il', Y, T, Y[:n, :].T), R[:n, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0a332-c1d7-4317-9849-0cf210cff00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testQR(randMatrices, blockedHouseholderQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50e995a-c3f6-48fe-9ff3-a47bac28df3d",
   "metadata": {},
   "source": [
    "Now, let us introduced tall-skinny QR(TSQR). This section is based on [Communication-optimal Parallel and Sequential QR and LU Factorizations](https://epubs.siam.org/doi/10.1137/080731992).\n",
    "\n",
    "Idea: tile along the rows of a tall matrix so we only need to process matrices that are approximately square one at a time. This improves the use of tensor engines and cache.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "    A_{1}\\\\\n",
    "    A_{2}\\\\\n",
    "    A_{3}\\\\\n",
    "    A_{4}\n",
    "\\end{bmatrix} \n",
    "&= \\begin{bmatrix}\n",
    "    Q_{11}R_1\\\\\n",
    "    Q_{12}R_2\\\\\n",
    "    Q_{13}R_3\\\\\n",
    "    Q_{14}R_4\n",
    "\\end{bmatrix} \n",
    "= \\begin{bmatrix}\n",
    "    Q_{11}\\\\\n",
    "    & Q_{12}\\\\\n",
    "    & & Q_{13}\\\\\n",
    "    & & & Q_{14}\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "    R_1\\\\\n",
    "    R_2\\\\\n",
    "    R_3\\\\\n",
    "    R_4\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "    Q_{11}\\\\\n",
    "    & Q_{12}\\\\\n",
    "    & & Q_{13}\\\\\n",
    "    & & & Q_{14}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "    Q_{21}\\\\\n",
    "    Q_{22}\\\\\n",
    "    Q_{23}\\\\\n",
    "    Q_{24}\n",
    "\\end{bmatrix} R \\\\\n",
    "&= \\begin{bmatrix}\n",
    "    Q_{11}Q_{21}\\\\\n",
    "    Q_{12}Q_{22}\\\\\n",
    "    Q_{13}Q_{23}\\\\\n",
    "    Q_{14}Q_{24}\n",
    "\\end{bmatrix} R = QR\n",
    "\\end{align*}$$\n",
    "\n",
    "Matrix $R$ can be computed recursively if it is still tall. The base case of the recursive call can be when the matrix is reduced enough so it fits in the Tensor core. Thus, tiling along the rows can be done multiple times to reduce matrix height during panelQR. This process produces a thin QR factorization and guarentees a globally orthonormal $Q$ matrix as it is a product of multiple orthonormal $Q$ factors.\n",
    "\n",
    "Notice that the $Q$ factors do not need to be explicitly contracted if an explicit $Q$ matrix is not needed. However, for our purpose, an explicit $Q$ matrix needs to be generated. The reason as to why an explicit $Q$ is necessary will be revealed later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e956f-47f3-465a-b7b9-4a6efa13a6a6",
   "metadata": {},
   "source": [
    "`tsqr` involves straightforward recursive decomposition and matrix multiplication. The Einsum cascade is simple so we will skip showing `tsqr` in terms of Einsum cascades here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea7b024-e2be-462a-92a9-73b61cbade59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsqr(A):\n",
    "    \"\"\"Performs TSQR.\"\"\"\n",
    "    m, n = A.shape\n",
    "    if m > n * 2:\n",
    "        # Tiles the input matrix horizontally by two\n",
    "        Q_11, R_1 = tsqr(A[:m // 2, :])\n",
    "        Q_12, R_2 = tsqr(A[m // 2:, :])\n",
    "\n",
    "        # Factorize R recursively to ensure global orthogonality\n",
    "        R = np.vstack((R_1, R_2))\n",
    "        Q_R, R_R = tsqr(R)\n",
    "\n",
    "        # Update the Q matrix\n",
    "        Q_1 = Q_11 @ Q_R[:Q_11.shape[1], :] # np.einsum(\"ij,jk->ik\", Q_11, Q_R[:Q_11.shape[1], :])\n",
    "        Q_2 = Q_12 @ Q_R[Q_11.shape[1]:, :] # np.einsum(\"ij,jk->ik\", Q_12, Q_R[Q_11.shape[1]:, :])\n",
    "    \n",
    "        return np.vstack((Q_1, Q_2)), R_R\n",
    "\n",
    "    Q, R = blockedHouseholderQR(A)\n",
    "    return Q, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f499e-f685-41bb-b119-eeeb1dd28d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testQR(randMatrices, tsqr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c12278-e9f9-4eff-98f7-f0a129d8d6ae",
   "metadata": {},
   "source": [
    "Notice if an input matrix is large, then a block of matrix `blockedHouseholderQR` is tall and skinny. This makes TSQR, a type of communication avoiding QR factorization(CAQR) algorithm, a good fit when factoring blocks of matrices. However, TSQR produces implicit tree-structed $Q$ factors as shown in [Communication-optimal Parallel and Sequential QR and LU Factorizations](https://epubs.siam.org/doi/10.1137/080731992). Managing the implicit $Q$ matrix can be complicated so an explicit $Q$ is computed in `TSQR`.\n",
    "\n",
    "Another problem arises when we consider trailing matrix update. While theoretically $A=QR$, updating the trailing matrix using an explicit $Q$ is numerically unstable and introduces orthogonality loss. Thus, we will need to reconstruct the CWY representation of the Householder product from the result of TSQR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0b5df0-e2b6-4ced-9348-f8bff4aecb86",
   "metadata": {},
   "source": [
    "The process of reconstruction is the most important part of this notebook, and it will be explained briefly. The reconstruction of the CWY representation from the result of TSQR is detailed more clearly in [Reconstructing Householder Vectors from Tall-Skinny QR](https://ieeexplore.ieee.org/document/6877344).\n",
    "\n",
    "Let $A$ be the matrix of interest for decomposition. The key idea in the paper is that LU decomposition can be applied to $A-R$. Recall that the CWY representation shows that it is possible to express $A$ in terms of \n",
    "$$A = (I-YTY^T)R$$\n",
    "so\n",
    "$$A - R = Y(-TY^TR).$$\n",
    "Since $Y$ is lower triangular while $T$, $Y^T$, and $R$ are upper triangular, this equation gives a unique LU decomposition of $A-R$. However, calling $\\text{LU}(A-R)$ is not numerically stable. Consider a low rank matrix $A$ (there are linearly dependent columns), the non-uniqueness of QR decomposition means that $(Q_{TSQR}, R_{TSQR})$ and $(Q_{Hh}, R_{Hh})$ may be different factor pairs. Then, performing LU decomposition on $A-R_{TSQR}$ in an attempt to recover Householder vectors that make up $Q_{Hh}$ is hopeless. Thus, for ill conditioned matrices, this method can be unstable.\n",
    "\n",
    "We will now shift our focus to reconstructing the Householder products of an orthonormal matrix $A$. Let $S$ be the sign matrix to the diagonal elements of $Q$. If $Q$ is tall, $S$ will just cover the upper square part of $Q$. Notice that QR decomposition on an orthonormal matrix $A$ is almost unique. We are only left with sign ambiguities. Then, the key idea is to set $R=S$, where $A=QS.$ Since $S$ is a diagonal sign matrix corresponding to the sign choices made inside the Householder-QR algorithm, performing an LU decomposition on $A-S$ is equivalent to performing the Householder-QR.\n",
    "\n",
    "Thus, to reconstruct the Householder product for a sqaure orthogonal matrix $Q$, we will:\n",
    "Perform $\\text{LU}(Q-S)$, where $S$ is the sign matrix of the square block of $Q$. We will get $Q-S=LU$ in this step. $L$ is exactly the lower triangular Householder vector matrix $Y$.\n",
    "\n",
    "To solve for $T$, we can then compute $T = -US^{-1}Y^{-T}.$ Since $Y_1$ is lower triangular, we can solve for $Y^{-T}$ using triangular solve with multiple right hand sides.\n",
    "\n",
    "Finally, the algorithm will output the decomposition $A=(I-YTY^T)(SR)$. This process essentially finds $A=QR$ through TSQR, reconstructs $Q=(I-YTY^T)(S)$ via LU decomposition, then outputs the final decomposition in the Householder form $A=(I-YTY^T)(SR)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f9ab2d-7734-47ea-9d37-f895d4d3b4e0",
   "metadata": {},
   "source": [
    "The following equation summarizes the `tsqr_hr` function that works not just for an orthogonal square matrix but for any general matrix $A$.\n",
    "\n",
    "Given a tall matrix $A \\in \\mathbb{R}^{m \\times b}$ with $m \\gg b$, the goal is to compute the QR decomposition using TSQR and reconstruct the Householder product in Compact WY form.\n",
    "\n",
    "Several points and steps\n",
    "1. Let us first separate $A$, \n",
    "$$A =\\begin{bmatrix}A_1 \\\\ A_2\\end{bmatrix}, \\quad A_1 \\in \\mathbb{R}^{b \\times b}, \\quad A_2 \\in \\mathbb{R}^{(m-b) \\times b}.$$\n",
    "2. Compute QR for the lower block: $\\text{TSQR}(A_2) \\rightarrow A_2 = Q_1 R_1.$ Now we have\n",
    "$$A = \\begin{bmatrix}A_1 \\\\ Q_1 R_1\\end{bmatrix}$$\n",
    "Notice that\n",
    "$$A = \\begin{bmatrix}A_1 \\\\ Q_1 R_1\\end{bmatrix} =  \\begin{bmatrix}I_b &  \\\\ & Q_1\\end{bmatrix} \\begin{bmatrix}A_1 \\\\ R_1\\end{bmatrix} = \\begin{bmatrix}I_b &  \\\\ & Q_1\\end{bmatrix} Q_2 \\tilde R = \\begin{bmatrix}I_b &  \\\\ & Q_1\\end{bmatrix} \\begin{bmatrix} Q_2^{(1)}  \\\\ Q_2^{(2)} \\end{bmatrix} R = \\begin{bmatrix}Q_2^{(1)} \\\\ Q_1Q_2^{(2)} \\end{bmatrix} R = QR,$$\n",
    "where $Q_2^{(1)}$ and $Q_2^{(2)}$ are the upper and lower square matrix in $Q_2$.\n",
    "4. Stack the upper part of $A$ with $R_1$ and perform QR again: $$\\tilde{A} = \\begin{bmatrix} A_1 \\\\ R_1 \\end{bmatrix}, \\quad \\text{TSQR}(\\tilde{A}) \\rightarrow \\tilde{A} = Q_2 \\tilde{R}$$\n",
    "5. `modifiedLU` is called on $Q_2^{(1)}$ to reconstruct the Householder transform. So we now get $L = Y_1$ as well as $U$ and $S$.\n",
    "6. $R = S\\tilde{R}$\n",
    "7. Compute the correction factor for the lower block of $Y$: $$W = Q_2^{(2)} U^{-1}$$\n",
    "8. Compute the lower part of $Y$: $$Y_{\\text{lower}} = Q_1 W$$\n",
    "9. Construct the full Householder matrix: $$Y =\n",
    "   \\begin{bmatrix}\n",
    "   Y_1 \\\\ Y_{\\text{lower}}\n",
    "   \\end{bmatrix}$$\n",
    "10. Compute the Compact WY representation: $$T = -U S Y_1^{-T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ce3ac-29cc-400f-945b-b6b65dd1c032",
   "metadata": {},
   "source": [
    "Let us attempt to show the algorithm described above in terms of Einsum cascades.\n",
    "\n",
    "We start by describing the `modifiedLU` function in terms of Einsums cascades. The function takes an orthogonal matrix $Q^{I,M,B} \\in \\mathbb{R}^{m \\times b}$ as input.\n",
    "\n",
    "Index info:\n",
    "- Let $i$ take on the value of `i` in `for i in range(b)`. \n",
    "\n",
    "First, we initialize the following matrices:\n",
    "- $S_{n,b}$.\n",
    "- $U_{n,b}$.\n",
    "- $L^{I,M,B}_{0,m,b} = I_{m \\times b}$.\n",
    "\n",
    "$$S^{N,B}_{i,i} = -\\text{sign}(Q^{I,M,B}_{i,i,i})$$\n",
    "$$U^{N,B}_{i,i} = Q^{I,M,B}_{i,i,i} - S^{N,B}_{i,i}$$\n",
    "$$U^{N,B}_{i,b:b \\ge i+1} = Q^{I,M,B}_{i,i,b \\ge i+1}$$\n",
    "$$L^{M,B}_{m:m\\ge i+1,i}=Q^{I,M,B}_{i,m:m\\ge i+1,i}/U^{N,B}_{i,i}$$\n",
    "$$\n",
    "Q_{i+1,m,b}=\n",
    "\\begin{cases}\n",
    "Q_{i,m,b}-L^{M,B}_{m,i}U^{N,B}_{i,b}, & \\text{if } (m \\ge i + 1) \\wedge (b \\ge i + 1), \\\\\n",
    "Q_{i,m,b}, & \\text{else}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d12d8-2dba-474c-92fd-10f460c23697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modifiedLU(Q):\n",
    "    \"\"\"Performs LU factorization without pivoting on Q\"\"\"\n",
    "    m, b = Q.shape\n",
    "    S = np.zeros((b, b))\n",
    "    U = np.zeros((b, b))\n",
    "    L = np.eye(m, b)\n",
    "    \n",
    "    for i in range(b):\n",
    "        S[i, i] = -np.sign(Q[i, i])\n",
    "        U[i, i] = Q[i, i] - S[i,i]\n",
    "        U[i, i+1:b] = Q[i, i+1:b]\n",
    "        L[i+1:m, i] = Q[i+1:m, i] / U[i, i]\n",
    "        Q[i+1:m, i+1:b] -= np.outer(L[i+1:m, i], U[i, i+1:b]) # np.einsum(\"j,k->jk\", L[i+1:m, i], U[i, i+1:b])\n",
    "    \n",
    "    return L, U, S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fdcf4b-336e-481b-ad38-55bd84e0cc58",
   "metadata": {},
   "source": [
    "To reconstruct the Householder representation, `tsqr_hr` will be used. The function takes any matrix $A^{M,B} \\in \\mathbb{R}^{m \\times b},$ performs TSQR or Householder QR, and returns the Householder representation of the transformation. The Einsum of `tsqr_hr` is given below:\n",
    "\n",
    "`if M < 2 * b:`, the input matrix is not tall so `blockedHouseholderQR` is used.\n",
    "\n",
    "`else`:\n",
    "\n",
    "`tsqr` is called on $A^*,$ where\n",
    "$${A^*}^{M,B}_{m,b}=A_{m: \\, m \\ge B, \\, b}$$\n",
    "producing\n",
    "$${A^*}_{m,b} = Q1_{m,p} R1_{p,b}.$$\n",
    "Then, `tsqr` is called on $\\tilde{A}$. $\\tilde{A}$ is defined as\n",
    "$$\\tilde{A}_{m,b} = \n",
    "\\begin{cases}\n",
    "A_{m, b:b}\n",
    "R1^{P,B}_{m, b}\n",
    "\\end{cases}$$\n",
    "After calling `tsqr`, we factorize $\\tilde{A}$ into\n",
    "$$\\tilde{A}_{m,b} = Q2_{m,l} \\tilde{R}_{l,b}.$$\n",
    "Using `modifiedLU` on $Q2^{M,L}_{m: m < L, \\, l}$, we get the decomposition along with the sign matrix:\n",
    "- $S^{N,Q}$\n",
    "- $U^{N,Q}$\n",
    "- $Y1^{M,Q}$\n",
    "\n",
    "$$R_{n,b} = S^{N,Q}_{n,l} \\tilde{R}_{l,b}$$\n",
    "\n",
    "To find $U^{-1}$, we use triangular solve. Let $U^{-1} = {U^{(-1)}}^{L,N}.$\n",
    "\n",
    "$$W^{M,N}_{m,n} = Q2^{M,L}_{m:m\\ge L, l}{U^{(-1)}}_{l,n}$$\n",
    "$${Y^{(\\text{lower})}}_{m,q}=Q1_{m,p}W^{M,N}_{p,q}$$\n",
    "$$Y_{m,q}=Y1_{m,q}+{Y^{(\\text{lower})}}_{m,q}$$\n",
    "\n",
    "Since $Y1^{M,Q}$ is lower triangular, we can find $Y1^{-T}$ using triangular solve. Let \n",
    "$Y1^{-T}={Y1^{(-T)}}^{Q,M}$\n",
    "$$T_{p,m} = - U^{N,Q}_{p,n} S_{n,q}{Y1^{(-T)}}_{q,m}$$\n",
    "\n",
    "The Householder representation is reconstructed and encoded in $Y$ and $T$.\n",
    "\n",
    "The final output of `tsqr_hr` includes\n",
    "- $T^{P,M}$\n",
    "- $Y^{M,Q}$\n",
    "- $R^{N,B}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d9778e-61f8-401e-91a1-c5b7d131419b",
   "metadata": {},
   "source": [
    "The Einsums for `triangular_matrix_inversion` is also given below. This function employs the standard back substitution algorithm.\n",
    "\n",
    "Consider the following definitions:\n",
    "- The input upper triangular matrix is $U \\in \\mathbb{R}^{M,N}$. Note that $M=N$.\n",
    "- $U^{-1}$ will be represented using $P$, where $P$ must also be upper triangular.\n",
    "\n",
    "Let $j$ iterate from $0$ to $N-1$\n",
    "$$P^{M,N}_{j,j} = 1/U^{M,N}_{j,j}$$\n",
    "\n",
    "Let $i$ iterate from $j-1$ to $0$.\n",
    "$$\\alpha_{i,j} = -U^{M,N}_{i,n:i+1 \\le n \\le j}P^{M,N}_{n,j}$$\n",
    "$$P^{M,N}_{i,j}=\\alpha_{i,j}/U^{M,N}_{i,i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378cf49-6413-4361-bcef-ce78bf0a795b",
   "metadata": {},
   "source": [
    "**Note that `ModifiedLU` and `triangular_inversion` need not be blocked as they are small matrices. Blocking them may only increase overhead, leading to a significant increase in flops without additional benefits.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89deeff4-f07c-424f-8f6b-c5708949b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangular_matrix_inversion(U):\n",
    "    \"\"\"Computes the inverse of an upper triangular matrix U using back-substitution.\"\"\"\n",
    "    n = U.shape[0]\n",
    "    U_inv = np.zeros((n,n))\n",
    "\n",
    "    for j in range(n):\n",
    "        U_inv[j, j] = 1.0 / U[j, j]\n",
    "        for i in range(j - 1, -1, -1):\n",
    "            U_inv[i, j] = -U[i, i+1:j+1] @ U_inv[i+1:j+1, j] / U[i, i]\n",
    "    return U_inv\n",
    "\n",
    "def tsqr_hr(A, Y, T, k):\n",
    "    \"\"\"Performs TSQR and reconstructs the Householder representation\"\"\"\n",
    "    m, b = A.shape\n",
    "\n",
    "    # The standard tsqr_hr only works for tall matrices such that the (number of rows) > (number of columns * 2)\n",
    "    if m < 2 * b:\n",
    "        Y[:, :], T[:b, :b], A[:b,:] = blockedHouseholderQR(A, householder_repr = True)\n",
    "    Q1, R1 = tsqr(A[b:, :])\n",
    "    Q2, A[:b,:] = tsqr(np.vstack([A[:b, :], R1]))\n",
    "    Y[:b, :b], U, S = modifiedLU(Q2[:b, :])\n",
    "    \n",
    "    U_inv = triangular_matrix_inversion(U)\n",
    "    # U_inv = np.linalg.inv(U)\n",
    "    \n",
    "    W = Q2[b:, :] @ U_inv # np.einsum(\"ij,jk->ik\", Q2[b:2*b, :], U_inv)\n",
    "    Y[b:, :] = Q1 @ W # np.einsum(\"ij,jk->ik\", Q1, W)\n",
    "\n",
    "    Y1_T_inv = triangular_matrix_inversion(Y[:b, :b].T)\n",
    "    # Y1_T_inv = np.linalg.inv(Y[:b, :b].T)\n",
    "    \n",
    "    T[:, :] = -U @ S @ Y1_T_inv # -np.einsum(\"ij,jk,kl->il\", U, S, Y1_T_inv)\n",
    "    A[:b,:] = S @ A[:b,:] # np.einsum(\"ij,jk->ik\", S, R_tilde)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac415a27-b77b-41af-a2c0-9f9ef7b73bac",
   "metadata": {},
   "source": [
    "To combine TSQR with our blocked Householder algorithm, we just need to call `tsqr_hr` for panel factorization and update the global matrix $Y$ and $T$.\n",
    "\n",
    "`BH_TSQR` share the same set of Einsum cascades as the the one shown in `blockedHouseholderQR` with slight changes to indexing because `panelQR` and `tsqr_hr` both handle the same task (construct CWY representation of the Householder product for a panel of input `A`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5032c177-35bc-4234-859f-669b23283ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BH_TSQR(A, block_size=25):\n",
    "    \"\"\"Performs QR factorization based on blocked Householder and TSQR.\"\"\"\n",
    "    m, n = A.shape\n",
    "\n",
    "    # tsqr_hr only works for tall matrices\n",
    "    if m < block_size * 2:\n",
    "        return blockedHouseholderQR(A)\n",
    "    \n",
    "    R = np.copy(A)\n",
    "    Y = np.zeros((m, n))\n",
    "    T = np.zeros((n, n))\n",
    "    \n",
    "    for k in range(0, n, block_size):\n",
    "        # Handle edge case\n",
    "        bk = min(block_size, n - k)\n",
    "        \n",
    "        # Compute a panel of the input matrix\n",
    "        tsqr_hr(R[k:, k:k+bk], Y[k:, k:k+bk], T[k:k+bk, k:k+bk], k)\n",
    "        \n",
    "        if k != 0:\n",
    "            # Compute T_cross\n",
    "            T_cross = -T[:k, :k] @ (Y[k:, :k].T @ Y[k:, k:k+bk]) @ T[k:k+bk, k:k+bk] # -np.einsum(\"ij,jk,kl,lm->im\", T[:k, :k], Y[k:, :k].T, Y[k:, k:k+bk], T_panel)\n",
    "            # Update the global T matrix\n",
    "            T[:k, k:k+bk] = T_cross\n",
    "\n",
    "        # Trailing matrix update\n",
    "        W = T[k:k+bk, k:k+bk].T @ Y[k:, k:k+bk].T @ R[k:, k+bk:] # np.einsum('ji,jk->ik', T[k:k+bk, k:k+bk], np.einsum('ji,jk->ik', Y[k:, k:k+bk], R[k:, k+bk:]))\n",
    "        R[k:, k+bk:] -= np.dot(Y[k:, k:k+bk], W) # np.einsum('ij,jk->ik', Y[k:, k:k+bk], W)\n",
    "\n",
    "    Q = np.eye(m, n) - Y @ T @ Y[:n, :].T # np.einsum(\"ij,jk,kl->il\", Y, T, Y[:n, :].T)\n",
    "\n",
    "    return Q, np.triu(R[:n, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b704358a-bb8d-41a8-86a5-ce386d494a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "testQR(randMatrices, BH_TSQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34f16d5-7f7f-4de1-b7f8-7dc35797c4db",
   "metadata": {},
   "source": [
    "We will perform some tests to compare our implementation with Numpy's implementation. Please note that our implementation is poorly optimized as Python does not provide direct access to more low level control of the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a5c8e-d328-42e5-bcd9-2b88b29a354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_qr(matrices, func):\n",
    "    \"\"\"Test to check if FUNC correctly computes the QR factorization for matrices in MATRICES.\"\"\"\n",
    "    totalTimeCost = 0\n",
    "    totalBackwardError = 0\n",
    "    totalOrthogonalityError = 0\n",
    "    \n",
    "    for matrix in matrices:\n",
    "        start = time.perf_counter()\n",
    "        q, r = func(matrix)\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        totalTimeCost += (end - start)\n",
    "        \n",
    "        backwardE = backwardError(matrix, q, r)\n",
    "        totalBackwardError += backwardE\n",
    "\n",
    "        orthogonalityE = orthogonalityError(q)\n",
    "        totalOrthogonalityError += orthogonalityE\n",
    "\n",
    "        isUpperTriangular = is_upper_triangular(r)\n",
    "\n",
    "        # Tolerance levels are set to 1e-12 although the errors will not usually be this large\n",
    "        if not (backwardE < np.float64(1e-12) and orthogonalityE < np.float64(1e-12) and isUpperTriangular):\n",
    "            print(\"Test Failed\")\n",
    "            return\n",
    "\n",
    "    print(\"Time taken:\" + str(totalTimeCost/len(matrices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a80140-867b-430f-8298-5faf8799dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(40):\n",
    "    randMatrix = [np.random.rand(10000 + 1000 * i, 1000)]\n",
    "    print(randMatrix[0].shape)\n",
    "    test_qr(randMatrix, np.linalg.qr)\n",
    "    test_qr(randMatrix, BH_TSQR)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
